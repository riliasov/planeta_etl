import sys
import os
import pandas as pd
import sqlalchemy
import traceback

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from src.config import load_config
from src.sheets import get_sheets_client, read_sheet_data
from src.etl.loader import DataLoader
from src.etl.data_cleaner import clean_dataframe
from src.utils.infer_schema import clean_column_name
from src.logger import get_logger

logger = get_logger(__name__)

def run_historical_sync():
    logger.info("üìö –ó–∞–ø—É—Å–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –¥–∞–Ω–Ω—ã—Ö (Historical Sync)...")
    
    config = load_config()
    db_url = config.get('SUPABASE_DB_URL')
    
    if not db_url:
        logger.info("‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î")
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    gc = get_sheets_client(config)
    engine = sqlalchemy.create_engine(db_url)
    loader = DataLoader(engine)
    
    sources = config.get('SOURCES', {})
    
    # 1. Historical Sales
    if 'historical_sales' in sources:
        process_source(gc, loader, sources['historical_sales'], 'historical_sales', 'sales_hst')
        
    # 2. Historical Expenses
    if 'historical_expenses' in sources:
        process_source(gc, loader, sources['historical_expenses'], 'historical_expenses', 'expenses_hst')

    # 3. Historical Trainings
    if 'historical_trainings' in sources:
        process_source(gc, loader, sources['historical_trainings'], 'historical_trainings', 'trainings_hst')

    # 4. Clients Data (–æ–±—ã—á–Ω–æ —ç—Ç–æ –∏—Å—Ç–æ—Ä–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤)
    if 'clients_data' in sources:
        process_source(gc, loader, sources['clients_data'], 'clients_data', 'clients_hst')

def process_source(gc, loader, source_config, source_name, target_table):
    logger.info(f"\nüì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ {source_name} -> staging.{target_table}...")
    
    spreadsheet_id = source_config.get('spreadsheet_id')
    sheet_identifiers = source_config.get('sheet_identifiers', [])
    ranges = source_config.get('ranges', {})
    use_gid = source_config.get('use_gid', False)
    
    if not sheet_identifiers:
        logger.info("   ‚ö†Ô∏è –ù–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –ª–∏—Å—Ç–æ–≤")
        return

    # –î–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–∏—Å—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≥–æ–¥–∞–º)
    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º
    for sheet_id in sheet_identifiers:
        range_name = ranges.get(sheet_id)
        logger.info(f"   üìÑ –õ–∏—Å—Ç: {sheet_id}...")
        
        try:
            data = read_sheet_data(gc, spreadsheet_id, sheet_id, range_name, use_gid)
            if not data or len(data) < 2:
                logger.info("      ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—É—Å—Ç–æ–π –ª–∏—Å—Ç")
                continue
                
            headers = data[0]
            
            # –£–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            seen = {}
            unique_headers = []
            for h in headers:
                h = str(h).strip()
                if h in seen:
                    seen[h] += 1
                    unique_headers.append(f"{h}_{seen[h]}")
                else:
                    seen[h] = 0
                    unique_headers.append(h)
            headers = unique_headers
            
            rows = data[1:]
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
            max_cols = len(headers)
            for row in rows:
                if len(row) > max_cols:
                    max_cols = len(row)
            
            if max_cols > len(headers):
                for i in range(len(headers) + 1, max_cols + 1):
                    headers.append(f"col_{i}")
                    
            padded_rows = []
            for row in rows:
                if len(row) < max_cols:
                    row = row + [None] * (max_cols - len(row))
                padded_rows.append(row)
                
            df = pd.DataFrame(padded_rows, columns=headers)
            
            # –ú–ê–ü–ü–ò–ù–ì –ö–û–õ–û–ù–û–ö
            rename_map = {}
            used_clean_names = {}
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è trainings_hst?
            # –í inferred_schema —É –Ω–∞—Å trainings_hst –∏–º–µ–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ (data, nachalo...),
            # –∑–Ω–∞—á–∏—Ç —Ç–∞–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—ã–ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, –∞ –Ω–µ –¥–∞—Ç—ã.
            # –ù–æ –ø—Ä–æ–≤–µ—Ä–∏–º, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Ç–∞–º —Ç–æ–∂–µ –¥–∏–Ω–∞–º–∏–∫–∞.
            # –°—É–¥—è –ø–æ —Å—Ö–µ–º–µ, —Ç–∞–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
            
            for col in df.columns:
                clean = clean_column_name(col)
                
                if clean in used_clean_names:
                    used_clean_names[clean] += 1
                    clean = f"{clean}_{used_clean_names[clean]}"
                else:
                    used_clean_names[clean] = 0
                
                rename_map[col] = clean
                
            df_renamed = df.rename(columns=rename_map)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            df_renamed['source_row_id'] = range(2, len(df_renamed) + 2)
            
            # –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•
            df_cleaned = clean_dataframe(df_renamed, target_table)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞
            loader.load_staging(df_cleaned, target_table, source_name)
            
        except Exception as e:
            logger.info(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Å—Ç–∞ {sheet_id}:")
            traceback.print_exc()

if __name__ == "__main__":
    run_historical_sync()
